# Algorithm Review
## Open and Closed Lists
- Think of open lists as frontiers of expansion
- We start with x<sub>0</sub> and expand neighbors until we reach x<sub>G</sub>
	- A*: expand frontier that our heuristic says will lead to minimum cost path
	- Dijkstra: expand frontier that is closest to x<sub>0</sub>
## A* Pseudocode
- Open list O = {x<sub>0</sub>}, closed list C = {}
- V(x<sub>0</sub>) = 0, V(x<sub>i</sub>) = $\infty$
- repeat until x<sub>G</sub> is in C
	- x<sub>j</sub>: vertex in O with lowest cost-to-come + heuristic
		- remove it from O and store it in C, closed list
	- for each neighbor x<sub>i</sub> of x<sub>j</sub> not already in C
		- compute V<sub>new</sub> = C(x<sub>j</sub>,x<sub>i</sub>) + V(x<sub>j</sub>)
		- update V(x<sub>i</sub>) if it is more than V<sub>new</sub>
		- if not in O, then add to O
## What's a Good Heuristic?
- heuristic gives an overestimate of actual V(x<sub>G</sub>)
	- h(x<sub>j</sub>)
## Admissible heuristic
- a heuristic is called an admissible heuristic if and only if h(x<sub>j</sub>)<= minimum cost to reach x<sub>j</sub> from x<sub>j</sub>  for all  x<sub>j</sub> 
- A* will find the optimal solution as long as you have an admissible heuristic
	- Actually, it also needs to be consistent (satisfy the triangle inequality)
		- h( x<sub>i</sub>) <= C(x<sub>i</sub>, x<sub>j</sub>) + h(x<sub>j</sub>); h(x<sub>G</sub>) = 0
- Special case for Manhattan Distance
	- Only valid if the diagonal isn't allowed (The diagonal would be less than the heuristic, causing it to oversestimate)
	- Function being used as an admissible heuristic depends on your actual application 
	- Use Euclidean Distance in most 2D cases 
## Good Heuristics
- Needs to be an underestimate & satisfy triangle inequality to guarantee we find optimal path
- The closer your heuristic is to actual estimate, the faster you will find the optimal path
	- need fewer expansions
- The closer your heuristic is to 0, the slower your algorithm
	- approaches Dijkstra; more expansions
## Analyzing Algorithms
1. Completeness: 
	1. An algorithm is complete if it is guaranteed to find a solution when one exists. In search or problem-solving contexts, completeness means the algorithm will not miss any possible solutions.
	2. DP, Dijkstra, A* are complete
2. Optimality: 
	1. An algorithm is optimal if it always finds the best possible solution according to a given criterion (such as shortest path, lowest cost, etc.). Optimality ensures the solution is not just any solution, but the most desirable one.
	2. DP, Dijkstra, A* are optimal
3. Efficiency: 
	1. Efficiency refers to the resources (like time and memory) that an algorithm uses to solve a problem. Efficient algorithms solve problems quickly and with minimal resource consumption, often measured in terms of time complexity (e.g., O(n), O(log n)) and space complexity.
## Optimality vs. Efficiency
- Sometimes you want a "good-enough" solution as fast as possible
- Can we trade-off optimality and efficiency?
## Weighted A*
- Dijkstra
	- expand based on lowest V(x<sub>i</sub>)
- A*
- Weighted A*
	- $\epsilon$ >= 1
	- Expand based on inflated heuristic
		- V(x<sub>i</sub>) + $\epsilon$ h(xi)
	- Can we guarantee optimality? 
		- NO!
		- However, we can guarantee that it will find a path who cost is no more than $\epsilon$ times the minimum cost path
		- V'(x<sub>G</sub>) <= $\epsilon$ V*(x<sub>G</sub>)
		- Question for homework!!!!!!!!!!
			- Prove whether or not the above statement is correct/incorrect (V'(x<sub>G</sub>) <= $\epsilon$ V*(x<sub>G</sub>))
## Anytime Algorithm
- If we stop the algorithm at any point in time, we should be able to return a good solution.
- More time we give an algorithm, the closer to the optimal the returned solution should be.
- Are DP, Dijkstra, A* anytime algorithm?